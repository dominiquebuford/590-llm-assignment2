{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiquebuford/590-llm-assignment2/blob/main/LLM_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpMJ6pERZvZz"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zopG5IdSe1qv",
        "outputId": "5df69c52-c22b-4c3f-ef6f-dd945f0d29cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 1s (189 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting poppler-utils\n",
            "  Downloading poppler_utils-0.1.0-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from poppler-utils) (8.1.7)\n",
            "Installing collected packages: poppler-utils\n",
            "Successfully installed poppler-utils-0.1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# use this command to install open cv2\n",
        "%pip install opencv-python\n",
        "\n",
        "# use this command to install PIL\n",
        "%pip install Pillow\n",
        "\n",
        "%sudo apt-get install poppler-utils\n",
        "%pip install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goKmLuzi7irv"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hWmuzIkeMN4",
        "outputId": "6e6f735e-09df-4e77-c66d-bc7b1724d693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/239.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpWkXgFq4qRE"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_chunk(page_text):\n",
        "  paragraphs = page_text.split('\\n\\n')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  tokenized_paragraphs = [tokenizer(paragraph, return_tensors='pt')['input_ids'].squeeze().tolist() for paragraph in paragraphs]\n",
        "  filtered_tokens = [_ for _ in tokenized_paragraphs if set(_) != set([101, 102])]\n",
        "  tokens = filtered_tokens[0]\n",
        "  print(**tokens)\n",
        "  chunks = []\n",
        "  MAX_TOKENS =  200\n",
        "  tokenCount = 0\n",
        "  current_chunk = []\n",
        "  for paragraph in filtered_tokens:\n",
        "    if tokenCount + len(paragraph) > MAX_TOKENS:\n",
        "      chunks.append(current_chunk)\n",
        "      tokenCount = 0\n",
        "      current_chunk = []\n",
        "    if len(paragraph) > MAX_TOKENS:\n",
        "      start_idx = 0\n",
        "      while start_idx < len(paragraph):\n",
        "        end_idx = min(start_idx + MAX_TOKENS-2, len(paragraph))\n",
        "        split_chunk = paragraph[start_idx:end_idx]\n",
        "        if split_chunk[0]!= 101:      #ensure the chunk has the [CLS] token at the beginning\n",
        "          split_chunk.insert(0, 101)\n",
        "        if split_chunk[-1]!= 102:\n",
        "          split_chunk.append(102)\n",
        "        chunks.append(split_chunk)\n",
        "        if end_idx == len(paragraph):\n",
        "          start_idx = end_idx\n",
        "        else:\n",
        "          start_idx = end_idx - 20\n",
        "      continue\n",
        "    current_chunk.extend(paragraph)\n",
        "    tokenCount += len(paragraph)\n",
        "\n",
        "  if len(current_chunk)>0:\n",
        "    chunks.append(current_chunk)\n",
        "\n",
        "  return chunks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO1kVu9ejQqn"
      },
      "outputs": [],
      "source": [
        "def tokenize_chunk_embed(text):\n",
        "    #paragraphs = text.split('\\n\\n')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    max_chunk_size = 512  # Choose your desired chunk size\n",
        "\n",
        "    # Tokenize each sequence and chunk the tokens\n",
        "    chunked_sequences = []\n",
        "    embeddings = []\n",
        "\n",
        "    #for paragraph in paragraphs:\n",
        "    tokens_dict = tokenizer(text, return_tensors='pt')\n",
        "    # Attention mask is automatically added by the tokenizer\n",
        "    input_ids = tokens_dict['input_ids']\n",
        "\n",
        "    # Chunk the tokens\n",
        "    for i in range(0, input_ids.size(1), max_chunk_size-20):\n",
        "      chunk = input_ids[:, i:i + max_chunk_size]\n",
        "      chunked_sequences.append(chunk)\n",
        "\n",
        "      # Process each chunk\n",
        "    for chunk in chunked_sequences:\n",
        "      # Forward pass through BERT model\n",
        "      with torch.no_grad():\n",
        "        chunk_outputs = model(input_ids=chunk)\n",
        "        last_hidden_states = chunk_outputs.last_hidden_state\n",
        "\n",
        "        single_embedding = torch.mean(last_hidden_states, dim=1).view(-1)\n",
        "        embeddings.append(single_embedding)\n",
        "\n",
        "    return chunked_sequences, embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1VxDrMteGb4",
        "outputId": "47f091ab-c0a7-4cc8-d444-7ed08498b19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1933 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1755 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1266 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "allChunks = []\n",
        "allEmbeddings = []\n",
        "pdf_folder_path = \"/content/drive/MyDrive/Masters-2/590-LLM/Homework/Assignment_2/wordDocs/test\"\n",
        "files = os.listdir(pdf_folder_path)\n",
        "for file in files:\n",
        "# Open and read the content of the Word document\n",
        "  path = os.path.join(pdf_folder_path, file)\n",
        "  doc = Document(path)\n",
        "  doc_content = \"\"\n",
        "  print(len(doc.paragraphs))\n",
        "\n",
        "  for paragraph in doc.paragraphs:\n",
        "    fileChunks, fileEmbeddings = tokenize_chunk_embed(paragraph.text)\n",
        "    allChunks.extend(fileChunks)\n",
        "    allEmbeddings.extend(fileEmbeddings)\n",
        "\n",
        "\n",
        "\n",
        "allChunks = [x[0] for x in allChunks]\n",
        "df_chunk_tokens = pd.DataFrame({'chunks': allChunks,  'embeddings': allEmbeddings})\n",
        "\n",
        "# Extract the embeddings from the output\n",
        "df_chunk_tokens.to_csv('/content/tokens_final.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri4f3qj6-lm2",
        "outputId": "44b45751-4010-4ae9-f233-013373c8badd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 1996,  5025,  8402,  1997,  2019,  3749,  1012,  1000, 14841, 20444,\n",
            "        28545,  3090,  2008,  1996,  3945,  2837,  2018,  2042,  1000,  2800,\n",
            "         2000, 20407,  2013,  1996,  2493,  1010,  1000,  1998,  1999,  2010,\n",
            "         3193,  1010,  2045,  2001,  2053,  1000,  8050, 28691,  1000,  2090,\n",
            "         1996,  2837,  1998,  1996, 17694,  1011,  2137,  2554,  1012,  2008,\n",
            "         5027,  1996,  2554,  5412,  1037,  3116,  2007,  1996,  3945,  2837,\n",
            "         1010,  1996, 13007,  2094,  2837,  1010, 18523,  6647, 23748,  1998,\n",
            "         7306,  9957,  3536,  9892,  1012,  2004, 14841, 20444, 28545,  3090,\n",
            "         1010, 20380,  2015,  2018,  2042,  2081,  1010,  2021,  2045,  2001,\n",
            "         2053,  2118,  3247,  2664,  1012, 13007,  2094,  7034,  1010,  1000,\n",
            "         8318,  2003,  5121,  9832,  2008,  1996,  2565,  2071,  3853,  6464,\n",
            "         2065,  1996,  2472,  2020, 21873,  2000,  1996,  2493,  1012,  1000,\n",
            "         4826,  1024,  1037,  2472,  2003,  3479,  1012,   102])\n",
            "in their preamble, the black studies program was not only to develop black appreciation among blacks but also to educate whites to the value and depth of black culture. also of concern was the educational deficiencies of the university in providing study in an area of crucial importance to contemporaryamerica. the program designed by the afro - american society. controversy over the black studies program dates back to the allen building takeover. [SEP]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(allChunks[3])\n",
        "text = tokenizer.decode(allChunks[23])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZUp5FIbACWZ",
        "outputId": "ee3d7c47-f838-48cf-d404-639c9dc383b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        }
      ],
      "source": [
        "print(len(allChunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyzBy9NxHLNF",
        "outputId": "b15a3c0c-3075-4fd5-a2bd-29a0aa70d78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-3.0.2-py3-none-any.whl (201 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/201.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m194.6/201.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Installing collected packages: pinecone-client\n",
            "Successfully installed pinecone-client-3.0.2\n"
          ]
        }
      ],
      "source": [
        "%pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiYIN6Ydr_vL"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"17944aa6-d5f5-48f6-843a-aa1e79415857\")\n",
        "index = pc.Index(\"590-llm-project\")\n",
        "i = 0\n",
        "for _, row in df_chunk_tokens.iterrows():\n",
        "  token_chunk = row['chunks']\n",
        "  embedding = row['embeddings'].numpy().tolist()\n",
        "  embedding_dict = {}\n",
        "  embeddingList = []\n",
        "  embedding_dict['id'] = str(i)\n",
        "  embedding_dict['values'] = embedding\n",
        "  embedding_dict['metadata'] = {}\n",
        "  embedding_dict['metadata']['chunk'] = tokenizer.decode(token_chunk)\n",
        "  embeddingList.append(embedding_dict)\n",
        "  index.upsert(vectors=embeddingList)\n",
        "  i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S85XEAc9DMfP",
        "outputId": "0848aeec-1bc4-44aa-9950-03662ce49653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.2774442434310913,-0.15314032137393951,-0.21152669191360474,-0.08726481348276138,0.23514285683631897,0.19983722269535065,0.2850814461708069,0.6772789359092712,-0.5113683938980103,0.16142429411411285,0.4447293281555176,0.035246334969997406,-0.20712542533874512,0.3810097873210907,-0.04219168424606323,0.12101281434297562,0.3331058919429779,-0.17929106950759888,-0.3411107063293457,-0.25397780537605286,-0.14534920454025269,-0.027893899008631706,-0.15765506029129028,0.5617544651031494,0.4859321713447571,-0.11596356332302094,0.19626472890377045,0.1243520975112915,0.0509086474776268,0.06480304151773453,0.12474337220191956,-0.05047542601823807,-0.41771718859672546,0.0937044769525528,-0.4670799672603607,-0.0724208876490593,-0.02533777616918087,0.09832016378641129,0.3169441223144531,0.3668022155761719,-0.942726731300354,-0.29119354486465454,-0.07140481472015381,0.25991758704185486,-0.4216510057449341,-0.24489018321037292,0.6027997136116028,-0.06896768510341644,0.3977562487125397,-0.3308320641517639,-0.20102809369564056,0.16404542326927185,0.25625425577163696,0.03821763023734093,-0.06446058303117752,0.24185125529766083,-0.04069020226597786,-0.48694008588790894,-0.24640648066997528,-0.6691741943359375,0.2995583713054657,0.23303252458572388,-0.41168928146362305,-0.7447789907455444,0.1925695687532425,0.4545972943305969,-0.011252712458372116,0.29763856530189514,-0.5703929662704468,-0.13282638788223267,-0.15104588866233826,-0.20075710117816925,0.028424246236681938,-0.346650093793869,-0.011290266178548336,-0.04744764417409897,0.10466953366994858,0.43673330545425415,0.16194146871566772,-0.42989447712898254,-0.1952790915966034,0.5986394286155701,-0.17937812209129333,0.2967521548271179,0.15070150792598724,0.04809291660785675,-0.19654631614685059,-0.061943016946315765,0.01247513946145773,0.3930835723876953,-0.34773004055023193,0.07317055761814117,-0.1485365480184555,0.22204691171646118,0.0032551821786910295,0.00049828109331429,0.07020851969718933,0.055633146315813065,-0.07167430222034454,0.28995195031166077,0.19440363347530365,-0.21456879377365112,0.2818249762058258,0.3196256458759308,-0.34418633580207825,-0.3627772033214569,0.3054226040840149,0.052587322890758514,-0.3582558035850525,0.13322225213050842,-0.45065373182296753,-0.4451283812522888,0.007448466960340738,-0.24078965187072754,-0.1456865668296814,0.309209406375885,0.07011118531227112,-0.03701777011156082,-0.16764119267463684,0.0610961988568306,-0.36069971323013306,-0.1373215913772583,0.11464761197566986,0.8250744938850403,-0.26599130034446716,0.46433785557746887,0.06768841296434402,0.1277587115764618,-0.2782420516014099,-0.37468332052230835,0.19572551548480988,0.4531373381614685,0.533478319644928,-0.01648806594312191,0.09011821448802948,0.12779046595096588,-0.04117646440863609,0.04785623401403427,-0.4114176630973816,-0.0816478580236435,0.07167705148458481,0.08214633911848068,-0.11255151778459549,-0.18369720876216888,0.01727101020514965,-0.4531930088996887,-0.09134499728679657,0.09385105222463608,0.4408988356590271,-0.02355494350194931,0.1423492580652237,0.014140776358544827,-0.17487415671348572,-0.2099003940820694,-0.07592583447694778,-0.33878692984580994,-0.535338282585144,0.380928099155426,0.08438095450401306,-0.238725483417511,0.43027788400650024,0.030246637761592865,0.08724477887153625,0.3077952265739441,-0.4962003827095032,0.5766773819923401,-0.2540339231491089,0.2528391182422638,-0.2659541964530945,0.24121159315109253,-0.34955209493637085,-0.0008722795755602419,0.5980530977249146,-0.06540394574403763,-0.3891250193119049,0.06719750910997391,0.252737432718277,-0.03203723952174187,0.3451676666736603,0.17641671001911163,-0.9616023302078247,-0.020499924197793007,0.19351570308208466,0.08696399629116058,-0.05325164273381233,-0.1316414475440979,-0.005947163328528404,-0.17146460711956024,-0.11525023728609085,-0.004086035769432783,0.1488194316625595,-0.26772913336753845,-0.28368309140205383,-0.11000097543001175,0.44144508242607117,-0.28818491101264954,-0.11743304133415222,-0.17106042802333832,-0.3940466642379761,-0.08009098470211029,0.3649102449417114,-0.19659782946109772,-0.06508201360702515,0.04618866741657257,-0.37875649333000183,0.028333516791462898,0.17121602594852448,0.349321186542511,-0.0014893091283738613,0.19778326153755188,-0.3990114629268646,0.13527491688728333,-0.08895334601402283,0.22466285526752472,0.09757571667432785,0.21620877087116241,0.01312814000993967,0.4133843183517456,0.06781956553459167,-0.34094011783599854,0.5157080292701721,-0.23734286427497864,0.02415427379310131,0.10328857600688934,0.05561530217528343,0.5331210494041443,0.07349810004234314,-0.1837434023618698,-0.05104181542992592,0.37669944763183594,0.225989431142807,0.15442818403244019,0.26266753673553467,0.4571707546710968,-0.14764176309108734,-0.20525261759757996,-0.34351974725723267,-0.17945612967014313,0.03649291768670082,-0.40333637595176697,-0.13130079209804535,0.21484416723251343,0.13533996045589447,0.32287895679473877,0.13659332692623138,-0.14482218027114868,0.5016777515411377,-0.08165370672941208,-0.1652052104473114,-0.44420120120048523,-0.601374089717865,-0.5859115123748779,-0.08473113924264908,-0.16945144534111023,-0.19544164836406708,0.1998024731874466,-0.107045978307724,0.053078074008226395,0.14974071085453033,0.06068052724003792,0.38800662755966187,0.3032906949520111,0.4886312186717987,0.3307586908340454,-0.45874544978141785,-0.5337405800819397,0.01549159362912178,0.3711976706981659,0.08835164457559586,-0.12640759348869324,-0.25303760170936584,0.04989147558808327,0.39668720960617065,0.19493447244167328,-0.57016921043396,0.040820617228746414,0.24262775480747223,-0.16026781499385834,-0.20997168123722076,-0.36792585253715515,-0.170272558927536,0.5254157185554504,-0.9056049585342407,-0.30371636152267456,0.2635036110877991,0.040661461651325226,-0.04334067553281784,-0.3206631541252136,0.2697082757949829,-0.218536838889122,-0.2606426775455475,0.2843341529369354,-0.161049023270607,-0.6486669778823853,0.02259514108300209,0.03036622330546379,0.12251762300729752,-0.0205371156334877,0.5422792434692383,-0.014410103671252728,-0.12839113175868988,-0.00039534844108857214,-0.35994651913642883,0.17935675382614136,-0.3252981901168823,0.04589388146996498,0.13004079461097717,-0.06249076873064041,-3.526360034942627,0.30440589785575867,0.020976828411221504,-0.22110363841056824,0.0594530925154686,-0.3662760555744171,-0.10213945806026459,0.25722435116767883,-0.6141940355300903,-0.1432715505361557,0.06426563113927841,-0.5772008299827576,-0.028777824714779854,0.794377326965332,0.22772666811943054,0.23266853392124176,0.6418986916542053,-0.5750817060470581,-0.18883730471134186,0.5454410910606384,0.0541236586868763,-0.4043200612068176,-0.030762992799282074,0.08495832234621048,0.3959684371948242,0.9164014458656311,-0.3471195101737976,-0.10988756269216537,-0.42070451378822327,-0.2776655852794647,-0.17361170053482056,-0.13069869577884674,0.24262462556362152,0.3151196837425232,0.08384622633457184,-0.029890257865190506,-0.14233443140983582,-0.2021314799785614,-0.8378608226776123,0.03479054197669029,0.04279989004135132,-0.22246679663658142,-0.5611370801925659,-0.02756563201546669,0.8071562051773071,0.041687946766614914,-0.049259863793849945,-0.34010758996009827,0.10829801857471466,-0.2584872543811798,-0.2634565830230713,0.28121230006217957,-0.041236620396375656,0.23752538859844208,-0.319856196641922,0.05900639668107033,0.21798011660575867,0.5670381188392639,0.03606828674674034,-0.5119355916976929,0.489624947309494,-0.2560654282569885,-0.06265958398580551,-0.010615264065563679,-0.505707323551178,-0.6320482492446899,-1.0734926462173462,-0.03847469761967659,0.3120827376842499,0.3537754714488983,0.11431308835744858,0.15775936841964722,-0.41926270723342896,-0.9341865181922913,-0.5490772724151611,0.2030593752861023,0.18770082294940948,-0.1982872635126114,-0.016892822459340096,0.1378317028284073,-0.06728531420230865,-0.11291246861219406,0.16152280569076538,0.048792190849781036,-0.025231700390577316,-0.43008947372436523,-0.053293414413928986,0.18844400346279144,-0.3903749883174896,-0.16145099699497223,0.29576176404953003,0.36743423342704773,0.39311277866363525,-0.0036839842796325684,0.1021554172039032,0.18813151121139526,0.40924474596977234,0.10295959562063217,0.18622902035713196,0.1901261806488037,-0.03568963706493378,-0.2267162948846817,0.3438912630081177,0.011827065609395504,0.07393267750740051,-0.09416176378726959,-0.5857653021812439,0.37609365582466125,0.39705708622932434,0.2137737274169922,0.16232405602931976,-0.7863674163818359,0.1579677164554596,-0.20163019001483917,0.36666807532310486,0.0612022802233696,0.2991844713687897,0.7987747192382812,0.14510852098464966,0.30890512466430664,0.22154560685157776,0.8257423043251038,-0.0790206640958786,-0.4559294879436493,-0.48989951610565186,-0.08281095325946808,-0.39716213941574097,0.07007864862680435,-0.3960086703300476,-0.20395320653915405,-0.16763527691364288,-0.2585356533527374,-0.14812542498111725,0.26582157611846924,0.0887163057923317,-0.1113889068365097,0.2036643922328949,-0.10118883103132248,-0.0793507993221283,-0.06781265139579773,0.1051998883485794,0.04911975562572479,0.04835481941699982,-0.128642737865448,0.2983422577381134,0.429451584815979,-0.06996463239192963,0.0511469841003418,0.02206992730498314,0.36484494805336,-0.16015490889549255,-0.45152193307876587,0.18129993975162506,-0.2330239862203598,-0.3521946370601654,0.039524078369140625,0.4219939410686493,-0.1731412708759308,-0.197610005736351,-0.20946763455867767,-0.07652597874403,0.2481735497713089,0.269325315952301,0.286316454410553,-0.16928356885910034,0.4097590148448944,0.16152334213256836,0.15812134742736816,-0.1747446209192276,0.37016963958740234,0.08057772368192673,-0.5768085718154907,0.1530395895242691,-0.12366639822721481,-0.011131305247545242,0.1887119859457016,0.00019471003906801343,-0.05296260118484497,-0.16349738836288452,0.1339140236377716,0.0009245299152098596,-0.1923418939113617,-0.11607770621776581,-0.19420067965984344,0.5535639524459839,0.23263677954673767,-0.11536774039268494,-0.08925946801900864,0.20879149436950684,-0.12908105552196503,-0.160032257437706,0.05948379263281822,0.027062498033046722,-0.5309603810310364,-0.5393401980400085,-0.2575432062149048,0.08460072427988052,-0.602345883846283,0.19636312127113342,0.44959667325019836,0.48871469497680664,0.1665513813495636,0.14842107892036438,0.3663509786128998,0.24333485960960388,0.20690110325813293,0.000274245860055089,-0.3141193985939026,-0.20573116838932037,0.009893197566270828,-0.33843740820884705,0.05110403150320053,-0.620290994644165,0.09901714324951172,0.09185592830181122,0.13731367886066437,-0.12494509667158127,-0.28644272685050964,-0.2366645336151123,0.4302971661090851,-0.5302953124046326,-0.11492074280977249,0.17791050672531128,0.0788693055510521,-0.40362387895584106,-0.41078245639801025,-0.3558259606361389,-0.05568229779601097,-0.6818088889122009,-0.061540134251117706,0.1955106556415558,-0.5152928233146667,-0.40836870670318604,0.08730920404195786,0.26518934965133667,0.30665987730026245,0.33835703134536743,-0.6383514404296875,0.26792171597480774,0.1257983148097992,-0.1370522677898407,-0.2259821742773056,-0.20999974012374878,-0.2919444143772125,0.21174924075603485,0.31702476739883423,0.06174171343445778,0.47834715247154236,0.026188025251030922,0.11610745638608932,-0.39307472109794617,0.005808770656585693,-0.6132764220237732,-0.46343785524368286,-0.26271671056747437,0.20982807874679565,0.03225380554795265,-0.3749139904975891,-0.015902768820524216,-0.0363648384809494,-0.06816421449184418,0.1386120319366455,-0.26171571016311646,0.039589133113622665,-0.033871810883283615,0.04153289645910263,0.39112937450408936,0.4176686108112335,0.6800734400749207,-0.3205842971801758,-0.3655738830566406,0.04852599278092384,0.042995695024728775,-0.005778891500085592,-0.03187980502843857,-0.07095488905906677,0.04386918246746063,-0.5867283344268799,0.1787368655204773,-0.24597349762916565,-0.21393412351608276,0.3226088285446167,-0.1225549727678299,0.21625739336013794,0.2665521204471588,0.07406313717365265,-0.16552090644836426,0.3984149098396301,-0.4893776774406433,-0.005681258160620928,0.2953084409236908,0.28632137179374695,0.09061433374881744,0.27683204412460327,-0.2054956555366516,0.19921153783798218,0.6315015554428101,-0.021733751520514488,-0.1464378982782364,-0.15411505103111267,-0.23084910213947296,0.3040574789047241,0.041264571249485016,0.14760857820510864,-0.015339771285653114,0.025959743186831474,0.20032468438148499,-0.1772013008594513,0.05927365645766258,-0.05611502379179001,0.06512020528316498,-0.16818013787269592,0.19679267704486847,0.5917707681655884,-0.2798907160758972,-0.43961700797080994,0.4200514554977417,-0.2660171687602997,0.2735767066478729,-0.2939050793647766,-0.16597703099250793,0.23820526897907257,0.34668391942977905,0.33967944979667664,0.27049583196640015,0.803581953048706,-0.22186830639839172,-0.10269283503293991,-0.2467077672481537,0.01219517458230257,-0.23380377888679504,0.3092881739139557,-0.1173938438296318,0.40070363879203796,-0.5264572501182556,-0.49803563952445984,0.10144142806529999,0.21222366392612457,-0.03252505511045456,0.3583661913871765,0.10554540902376175,0.3866254389286041,0.4380226731300354,0.20892208814620972,-3.156753518851474e-05,-0.09920693188905716,-0.0387980118393898,0.26360610127449036,0.474267840385437,0.2852989435195923,-0.11488404870033264,0.23257893323898315,0.09770727157592773,-0.17164741456508636,0.10925821959972382,0.21077050268650055,0.18192139267921448,0.3175016939640045,-0.27317649126052856,-0.16695068776607513,-0.10599644482135773,0.024051230400800705,0.4621739983558655,-0.8148373365402222,0.41318246722221375,0.25426146388053894,-0.1285153031349182,0.04337318614125252,-0.16939213871955872,0.12602069973945618,0.31427302956581116,0.11236537247896194,-0.16617679595947266,-0.5214797258377075,-0.20106884837150574,0.6487249135971069,0.07390903681516647,-0.34006020426750183,-0.22729478776454926,-0.08743923157453537,0.10068202018737793,-0.05136111006140709,-0.433103084564209,-0.01957697793841362,-0.2720293402671814,-0.10200349241495132,-0.16561366617679596,0.21667291224002838,-0.21635492146015167,-0.5268413424491882,0.09618760645389557,-0.33385974168777466,0.06838161498308182,-0.12694473564624786,0.18595655262470245,-0.22016564011573792,0.24819961190223694,0.032807715237140656,0.3250734508037567,-0.09297037869691849,0.5750230550765991,-0.3873729705810547,-0.606785774230957,0.16748635470867157,-0.09574037045240402,-0.0389154776930809,0.09159913659095764,0.1962132304906845,-0.40396812558174133,-0.01778222806751728,-0.09839500486850739,0.3948166072368622,-0.7018966674804688,-0.08879982680082321,0.20655693113803864,0.23596413433551788,0.17712034285068512,0.2028050720691681,-0.34768179059028625,0.2847921550273895,-0.4610055387020111,-0.2013479769229889,-0.23211057484149933,0.6426110863685608,0.08356309682130814,0.17121738195419312,0.1600308120250702,0.33362168073654175,-0.12848038971424103,0.12464100867509842,0.030063709244132042,0.04083241894841194,0.4110996127128601,-0.3156049847602844,0.2844586968421936,0.029808666557073593,0.04983852058649063,0.11967547237873077,-0.3019372820854187,-0.40661144256591797,0.0992729514837265,0.06691402196884155,0.26402828097343445,-0.15111002326011658,-0.08259054273366928,-0.379365473985672,0.09017612040042877,-0.24798010289669037,0.029137756675481796,-0.5808640718460083,-0.1900128573179245,-0.19007645547389984,-0.13536320626735687,-0.3101293742656708,0.014313887804746628,-0.17867207527160645,-0.4422092139720917,-0.0283639058470726,0.17575320601463318,0.09890580922365189\n"
          ]
        }
      ],
      "source": [
        "vector_example = \"How many Black students were enrolled at Duke in 1969?\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "tokens_dict = tokenizer(vector_example, return_tensors='pt')\n",
        "input_ids = tokens_dict['input_ids']\n",
        "with torch.no_grad():\n",
        "  chunk_outputs = model(input_ids)\n",
        "  last_hidden_states = chunk_outputs.last_hidden_state\n",
        "\n",
        "single_embedding = torch.mean(last_hidden_states, dim=1).view(-1)\n",
        "finallist = single_embedding.numpy().tolist()\n",
        "print(\",\".join(map(str, finallist)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X17YtT2CM_EF",
        "outputId": "f8043603-e3fc-47cc-da0d-09b72733d017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.407054603099823, 0.09991490840911865, 0.12122315168380737, -0.021417012438178062, 0.27619466185569763, -0.10729444772005081, 0.06951545923948288, 0.37169545888900757, -0.04063038155436516, -0.27731388807296753, 0.02180408500134945, -0.0891823098063469, -0.21054336428642273, 0.15984901785850525, -0.06740947812795639, 0.30978941917419434, 0.46039071679115295, -0.09118194133043289, -0.22701114416122437, 0.19456975162029266, 0.0693352073431015, 0.14152589440345764, 0.1185150220990181, 0.60248863697052, 0.42395809292793274, 0.08937353640794754, 0.019101794809103012, -0.1636626273393631, -0.20778045058250427, -0.0860336646437645, 0.643413245677948, -0.0065162512473762035, -0.16075043380260468, -0.29862669110298157, -0.10226297378540039, -0.0676744133234024, -0.005814490839838982, -0.2803872525691986, 0.011356900446116924, 0.14107143878936768, -0.4791681468486786, -0.15875573456287384, -0.30446305871009827, -0.0800161063671112, -0.18453054130077362, -0.17662476003170013, 0.37350088357925415, -0.13393473625183105, -0.11106966435909271, -0.11511273682117462, -0.5374747514724731, 0.38864004611968994, -0.061920952051877975, -0.030637642368674278, 0.194660484790802, 0.4360802471637726, -0.11923045665025711, -0.3123209476470947, -0.5522770285606384, -0.39567822217941284, 0.262954443693161, -0.05648307129740715, 0.19724971055984497, -0.3110906779766083, 0.026274878531694412, 0.048419199883937836, 0.0763791874051094, 0.280642032623291, -0.8101898431777954, -0.1554451733827591, -0.24646428227424622, -0.06042978912591934, -0.2004372775554657, -0.12889494001865387, -0.043251197785139084, -0.21179981529712677, 0.12073167413473129, 0.21593712270259857, 0.017840107902884483, 0.022902214899659157, -0.18085965514183044, 0.4483984112739563, -0.15838071703910828, 0.39557337760925293, -0.07912714034318924, -0.1808767318725586, 0.11097825318574905, 0.24079275131225586, -0.19562914967536926, 0.6420271396636963, -0.08582916110754013, -0.10692745447158813, 0.08965152502059937, 0.07740554213523865, 0.03696126863360405, -0.2943050265312195, 0.4980887472629547, -0.14871549606323242, -0.27984216809272766, 0.15028996765613556, 0.06841690093278885, -0.23517228662967682, 0.03920869529247284, 0.1092962846159935, -0.20345023274421692, -0.10814919322729111, 0.1343524158000946, 0.04231895133852959, -0.5009375810623169, 0.3233184814453125, 0.017375582829117775, -0.15303577482700348, -0.13539916276931763, -0.32324111461639404, 0.08679596334695816, 0.11035086214542389, 0.02809048630297184, -0.11761102825403214, -0.19303834438323975, 0.07638973742723465, 0.2187330722808838, 0.037445083260536194, 0.1579425036907196, 0.49098917841911316, -0.04870038479566574, -0.03036678023636341, 0.13095636665821075, 0.10445963591337204, -0.3918609917163849, -0.2678254246711731, 0.17762884497642517, 0.11005130410194397, 0.09245245158672333, -0.16288906335830688, -0.2209578901529312, 0.17084982991218567, -0.21868817508220673, -0.09984411299228668, -0.33504411578178406, 0.1716753989458084, 0.027607731521129608, -0.4012352228164673, 0.2457287311553955, -0.20818348228931427, 0.1535421758890152, -0.29597070813179016, 0.10604904592037201, -0.0014320212649181485, 0.021994508802890778, 0.3507520854473114, 0.3168851435184479, 0.01001819595694542, 0.042060647159814835, -0.0861462727189064, -0.07443831861019135, -0.09669070690870285, -0.4070427119731903, 0.3291987180709839, -0.037955403327941895, 0.0691489577293396, 0.3274446427822113, 0.17748811841011047, -0.2588515877723694, 0.17646054923534393, -0.021390661597251892, 0.34531062841415405, -0.28320834040641785, 0.2795639634132385, -0.09619259834289551, 0.33006399869918823, 0.0012285602279007435, -0.38044795393943787, 0.5996149778366089, -0.2025751769542694, 0.015877418220043182, 0.09929943829774857, 0.39009395241737366, -0.018809910863637924, 0.2322368621826172, -0.053250983357429504, -0.7669343948364258, 0.17967596650123596, 0.03407654911279678, 0.0001946223492268473, 0.04703498259186745, -0.1965537816286087, 0.008997181430459023, -0.2527659833431244, 0.32127225399017334, -0.2327479124069214, -0.05649755522608757, -0.5243303179740906, -0.32809504866600037, -0.08933895826339722, 0.3204628825187683, -0.11873100697994232, -0.04619147628545761, 0.1641223132610321, -0.3451750576496124, -0.11461470276117325, 0.15461692214012146, -0.02535177394747734, 0.2666674852371216, 0.10711925476789474, -0.004205035511404276, -0.21022209525108337, 0.09315041452646255, -0.14853698015213013, -0.0705660805106163, 0.25862228870391846, -0.019988177344202995, 0.21575990319252014, 0.07767419517040253, -0.009437954984605312, 0.1919872760772705, -0.004635154735296965, -0.03765668720006943, -0.090028315782547, 0.08918052911758423, -0.141580268740654, 0.15133628249168396, 0.18806128203868866, -0.42946895956993103, 0.36065036058425903, -0.04721342399716377, 0.588497519493103, -0.07502477616071701, -0.7822912931442261, 0.1747785359621048, 0.6563383340835571, -0.012737915851175785, -0.35404208302497864, 0.41047513484954834, -0.07485786080360413, 0.027044154703617096, -0.06982588022947311, -0.6103581190109253, -0.08823005110025406, 0.18594622611999512, -0.42635953426361084, -0.05959857627749443, 0.4076174199581146, 0.03176380321383476, 0.16685166954994202, -0.09320977330207825, -0.16187334060668945, 0.0774955078959465, -0.018532337620854378, -0.05853767693042755, -0.2018718123435974, -0.49375274777412415, -0.1345941722393036, -0.060222893953323364, -0.22376297414302826, -0.013567845337092876, 0.011477502062916756, 0.06649996340274811, -0.37967419624328613, 0.10253705829381943, -0.1887664496898651, 0.22305108606815338, 0.19860991835594177, 0.10924478620290756, 0.0602421872317791, -0.1556602269411087, -0.5056419372558594, 0.337541788816452, 0.18769994378089905, 0.051657188683748245, 0.11333344876766205, 0.11940038204193115, -0.08659620583057404, -0.12194495648145676, 0.4596174657344818, -0.46771156787872314, 0.1649656742811203, 0.41076773405075073, 0.018810084089636803, -0.20858128368854523, -0.3244188725948334, 0.1775856465101242, 0.3427354693412781, -0.39528465270996094, 0.07278213649988174, -0.05308063328266144, -0.09525030106306076, 0.3176776170730591, -0.29483872652053833, -0.03614821285009384, -0.16820617020130157, -0.3604530990123749, 0.14897100627422333, -0.3077569305896759, -0.3230416476726532, 0.16881193220615387, 0.14667163789272308, 0.12340377271175385, 0.143964022397995, 0.17767247557640076, 0.06771254539489746, -0.36533427238464355, -0.3920455276966095, 0.2542279362678528, 0.04937877878546715, -0.1485583782196045, 0.21218520402908325, -0.03508852422237396, -0.1514935940504074, -3.797830104827881, 0.17403870820999146, 0.03389061987400055, -0.21966665983200073, 0.09786011278629303, -0.0023671260569244623, -0.17116022109985352, -0.07771747559309006, -0.21452486515045166, 0.038741208612918854, -0.26132214069366455, -0.49918532371520996, 0.18760080635547638, 0.5986489057540894, 0.30245980620384216, -0.08357622474431992, 0.19109511375427246, -0.3640606701374054, -0.10929733514785767, 0.322012722492218, -0.2394249141216278, -0.7297952771186829, 0.16204231977462769, -0.133588045835495, 0.3518059253692627, 0.6636409759521484, -0.44601351022720337, -0.12691055238246918, -0.3845236301422119, -0.18475131690502167, 0.2798728942871094, -0.17989207804203033, 0.21585680544376373, 0.21804139018058777, 0.09274572879076004, 0.012825251556932926, 0.049738284200429916, -0.344466894865036, -0.294201523065567, -0.4438228905200958, -0.012571554630994797, -0.6434880495071411, -0.21824750304222107, -0.1250678151845932, 0.579400897026062, 0.11258133500814438, 0.02078286185860634, -0.2478438764810562, 0.21993465721607208, 0.16007527709007263, 0.024771982803940773, 0.35363438725471497, -0.25977736711502075, -0.006792232859879732, -0.005487785208970308, 0.0992126390337944, 0.3329056203365326, 0.23946194350719452, -0.4464861750602722, -0.2582460939884186, 0.0969456136226654, -0.24608920514583588, -0.2905285954475403, 0.12883301079273224, -0.2185102254152298, -0.3782350420951843, -0.4610643684864044, 0.1317499428987503, 0.1425943374633789, -0.01229211688041687, 0.020460791885852814, 0.6645011901855469, -0.3784494400024414, -0.2777882218360901, -0.0380999855697155, -0.3215011656284332, 0.33070167899131775, -0.15651202201843262, -0.046388059854507446, -0.16861647367477417, -0.07578516751527786, -0.3137245774269104, 0.03463942930102348, 0.14020049571990967, -0.013283279724419117, -0.2197258025407791, 0.2650846540927887, 0.14447806775569916, -0.39044898748397827, -0.3762063682079315, 0.2853397727012634, 0.12269323319196701, 0.1902305632829666, -0.06757435202598572, 0.1275230050086975, -0.23327556252479553, 0.24954041838645935, -0.1323683261871338, 0.04943302646279335, -0.19634753465652466, 0.030504629015922546, 0.016764797270298004, 0.24512013792991638, -0.07299425452947617, 0.0773668885231018, -0.08164546638727188, -0.2675977051258087, -0.002358419820666313, 0.2365666776895523, 0.09958198666572571, 0.2387673556804657, -0.43421223759651184, 0.39066198468208313, -0.4280092120170593, 0.13684888184070587, 0.07801581174135208, 0.22671344876289368, 0.6329671740531921, 0.007933943532407284, -0.02507413923740387, 0.08065011352300644, 0.3738172650337219, 0.06964090466499329, -0.23147054016590118, -0.2949991226196289, -0.028293631970882416, -0.23424990475177765, 0.04887290298938751, -0.02971368469297886, 0.1681424081325531, -0.17622599005699158, -0.14465302228927612, -0.2547740638256073, 0.28598713874816895, 0.21653413772583008, -0.062291812151670456, -0.02160794287919998, -0.37654730677604675, -0.08546819537878036, 0.18246875703334808, 0.02205248549580574, 0.2762698531150818, 0.044323407113552094, -0.1783941090106964, 0.05174028500914574, 0.47753703594207764, 0.1491767317056656, 0.26277464628219604, -0.2721690833568573, 0.1168818473815918, -0.47773507237434387, -0.21359869837760925, -0.024668602272868156, -0.12130572646856308, -0.1835872381925583, 0.28195157647132874, 0.161865696310997, -0.029296480119228363, -0.009495055302977562, -0.31231486797332764, 0.053347039967775345, 0.011065791361033916, 0.10239361226558685, 0.08297446370124817, -0.13130532205104828, 0.4560835361480713, -0.0038585022557526827, -0.07618331164121628, -0.11328647285699844, 0.06926964968442917, -0.1396031379699707, -0.33884289860725403, 0.03287096321582794, -0.34598469734191895, 0.060840778052806854, 0.2769163250923157, -0.02055487595498562, -0.28461697697639465, -0.07275852560997009, 0.28566908836364746, 0.14363932609558105, -0.21964579820632935, -0.2348308563232422, -0.17305780947208405, 0.45617949962615967, -0.015914909541606903, 0.18769697844982147, -0.39966049790382385, -0.07913251221179962, -0.12056651711463928, 0.00862355250865221, -0.008914461359381676, -0.015089182183146477, 0.15296822786331177, -0.36039701104164124, 0.04432537779211998, 0.277816504240036, -0.2496013343334198, 0.03968025743961334, 0.28931787610054016, 0.3663147985935211, -0.17845088243484497, -0.12594735622406006, 0.0167560912668705, 0.052679017186164856, -0.19774562120437622, 0.05823910981416702, 0.05279123783111572, -0.04666541889309883, 0.23616527020931244, -0.5177137851715088, 0.13738662004470825, -0.05497703328728676, 0.0677901953458786, 0.1607178896665573, -0.4686926007270813, -0.23318175971508026, -0.13908511400222778, -0.30129289627075195, -0.005164011847227812, -0.04953499510884285, -0.05449461564421654, 0.18445764482021332, -0.1326998919248581, -0.21791741251945496, -0.09173169732093811, 0.07473523169755936, 0.026151444762945175, -0.1673128753900528, -0.16130630671977997, 0.241617813706398, -1.0521515607833862, -0.10781973600387573, 0.21061751246452332, -0.1141623854637146, 0.18657858669757843, 0.12057586014270782, -0.5676126480102539, 0.06462889909744263, -0.06794217228889465, -0.1439475566148758, 0.08840767294168472, -0.2112659364938736, -0.04982703924179077, 0.16645295917987823, 0.07068318873643875, -0.10475091636180878, 0.3464643955230713, -0.017433129251003265, 0.5401463508605957, -0.028018925338983536, -0.07324624061584473, -0.17962132394313812, -0.22348688542842865, -0.016484493389725685, -0.1798490285873413, -0.1447029411792755, 0.016535256057977676, 0.013822540640830994, 0.05905480310320854, -0.014356224797666073, -0.30241331458091736, -0.22944195568561554, -0.025596357882022858, 0.2783413231372833, -0.010023180395364761, -0.04330521821975708, -0.016697824001312256, 0.3208087980747223, -0.6469802260398865, 0.17708006501197815, -0.09779270738363266, -0.32491692900657654, -0.21479405462741852, 0.18706385791301727, 0.16696734726428986, 0.041654299944639206, -0.48725780844688416, 0.4337845742702484, -0.6622442007064819, -0.30719083547592163, 0.007179379928857088, -0.014971994794905186, 0.12520577013492584, 0.4100923538208008, -0.40761521458625793, -0.04175552353262901, 0.42080727219581604, -0.1481049507856369, -0.20561400055885315, 0.3326106667518616, -0.19546250998973846, -0.007343322038650513, 0.041022006422281265, -0.5333689451217651, 0.007883906364440918, 0.609230101108551, 0.24664466083049774, -0.10641836374998093, 0.2311716079711914, 0.015068565495312214, -0.026573633775115013, 0.18963360786437988, 0.08460815250873566, -0.20697656273841858, 0.056527748703956604, 0.20925389230251312, -0.5071330070495605, -0.05587232485413551, 0.11064738035202026, 0.168117493391037, -0.2557832896709442, 0.5539939999580383, 0.3399488627910614, -0.34198832511901855, -0.2488572746515274, -0.17672763764858246, -0.08269133418798447, 0.2580227553844452, -0.13083630800247192, 0.2220996469259262, 0.13174636662006378, 0.44127708673477173, 0.05076305568218231, 0.14747239649295807, 0.27130910754203796, -0.07755020260810852, -0.11842048168182373, -0.10235851258039474, 0.30133336782455444, -0.047112029045820236, 0.5049677491188049, -0.19627945125102997, 0.2370195835828781, -0.1558298021554947, -0.12643592059612274, 0.10082009434700012, -0.19333773851394653, -0.053851429373025894, 0.20003104209899902, -0.02346860058605671, 0.19807560741901398, 0.6295871138572693, 0.23806272447109222, 0.33080142736434937, 0.17054718732833862, 0.3246363401412964, 0.022546984255313873, 0.6082461476325989, 0.4495738744735718, 0.10656146705150604, 0.143104687333107, -0.025968089699745178, -0.0527959018945694, 0.21071037650108337, 0.29633674025535583, 0.17897559702396393, 0.3598465621471405, -0.02130807563662529, 0.43384480476379395, -0.03775155171751976, 0.36569929122924805, 0.5667704939842224, -0.44729161262512207, 0.0028160042129456997, -0.06363844871520996, 0.06409161537885666, -0.2633558511734009, 0.08589582145214081, 0.009511910378932953, 0.21941789984703064, 0.16459180414676666, -0.3202460706233978, -0.34942537546157837, 0.0066398801282048225, 0.5557007193565369, 0.2740035057067871, -0.19955411553382874, 0.03868882358074188, 0.22212186455726624, 0.11883433163166046, -0.2613537311553955, -0.34087300300598145, -0.334179550409317, -0.23385123908519745, -0.30592474341392517, 0.17727908492088318, 0.08546144515275955, 0.005849356763064861, -0.21784429252147675, 0.023489784449338913, -0.14351710677146912, 0.1179402768611908, -0.0927063450217247, -0.009957324713468552, -0.10022778809070587, 0.4257010519504547, 0.024893565103411674, 0.6374799609184265, -0.12694357335567474, 0.17061959207057953, -0.17628663778305054, -0.2875233292579651, 0.13954414427280426, 0.21890002489089966, -0.0019138115458190441, 0.18133074045181274, 0.16754789650440216, -0.38131892681121826, 0.010415276512503624, -0.04966410622000694, 0.19124485552310944, -0.35830599069595337, 0.10055617988109589, 0.18329092860221863, 0.0004206319572404027, 0.007000371813774109, 0.3371153175830841, -0.24935297667980194, -0.10924287140369415, -0.19391778111457825, -0.3191104233264923, -0.15126653015613556, 0.3375130295753479, -0.30149057507514954, -0.2169993370771408, 0.1574086844921112, 0.09038222581148148, -0.02875715121626854, 0.007278334349393845, -0.04574914649128914, 0.006851839367300272, 0.00016548675193917006, 0.04934372380375862, 0.02323910780251026, -0.0860334262251854, -0.18888890743255615, 0.15003655850887299, -0.4364810585975647, 0.14773666858673096, 0.1547389179468155, 0.18937204778194427, -0.020673956722021103, -0.07491021603345871, 0.522689163684845, -0.15362559258937836, 0.05999993905425072, -0.24765455722808838, -0.1898469626903534, -0.20679914951324463, -0.2619137465953827, -0.19432540237903595, 0.28729701042175293, -0.307111918926239, -0.2039838284254074, -0.40050122141838074, -0.07570844143629074, 0.0249788798391819, -0.027193885296583176, 0.1914408653974533]\n",
            "[[0.6770785]]\n"
          ]
        }
      ],
      "source": [
        "print(allEmbeddings[21].numpy().tolist())\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarityscores= cosine_similarity(single_embedding.numpy().reshape(1, -1), allEmbeddings[23].numpy().reshape(1,-1))\n",
        "print(similarityscores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2qrcsyHYEto"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartModel\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "model = BartModel.from_pretrained('facebook/bart-large')\n",
        "\n",
        "inputs = tokenizer(\"what was the budget for the program in 1972?\", return_tensors=\"pt\")\n",
        "input_ids = inputs['input_ids']\n",
        "outputs = model(input_ids)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "n51jmHQhYR2a",
        "outputId": "67d9fe60-68b2-482d-cc90-c4720773e622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 1.0126,  0.1038, -1.7637,  ...,  0.9137,  0.5540,  0.5937],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "tensor([ 0.2615, -0.0307,  0.2343,  ...,  0.8356,  0.0234,  0.1768],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-37a693a4a8f6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdecoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3746\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3747\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_source_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         legacy_added_tokens = set(self._added_tokens_encoder.keys()) - set(self.all_special_tokens) | {\n\u001b[1;32m   1003\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_special_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMc5zpZzjdLWSB8/K8wBoUf",
      "include_colab_link": true,
      "mount_file_id": "1bHWIFFPqVDHL0A1A79ZAjdNFUoP0gv1B",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
